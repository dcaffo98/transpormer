# TranSPormer: a transformer for the Travelling Salesman Problem
This repository presents a proof-of-concept of a transformer neural network to address the Travelling Salesman Problem avoiding any auto-regressive component.
![Architecture overview](imgs/arch-comparison.png)

<br>
We rely on a cross-attention layer where the positional encodings represent the queries, while keys and values comes from the graph node embeddings.

![Attention matrix proposed](imgs/attn-matrix-proposed-bg.png)

<br>
An instance of an attention matrix from our model before and after training.

![Attention matrix before training](imgs/attention-matrix-no-train.png)
![Attention matrix after training](imgs/attention-matrix-train.png)

<br>

Some examples of tours generated by our model that are shorter than those produced by the Christofides algorithm from [NetworkX](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.approximation.traveling_salesman.traveling_salesman_problem.html).
![qualitative results](imgs/00.png)
![qualitative results](imgs/01.png)
![qualitative results](imgs/02.png)
![qualitative results](imgs/03.png)
![qualitative results](imgs/04.png)
![qualitative results](imgs/05.png)
![qualitative results](imgs/06.png)
![qualitative results](imgs/07.png)
![qualitative results](imgs/08.png)
![qualitative results](imgs/09.png)

<br>

### Create a dataset of 10k random graphs of 50 nodes each
`> python create_dataset.py --path ./my_dataset --n 10000 --n_nodes 50`